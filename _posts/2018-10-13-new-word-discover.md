---
layout: post
title: 新词发现
date: 2018-10-13 23:17
---

新词发现

<!-- more -->

> 来源：http://www.matrix67.com/blog/archives/5044

**信息熵**：
能够反映知道一个事件的结果后平均会给你带来多大的信息量。
如果某个结果发生的概率为 $p$，当时知道它确实发生了，你得到的信息量就被定义为 $-\log(p)$，$p$ 越小，你得到的信息量就越大。

假设两个随机变量 $x$ 和 $y$ 是相互独立的，那么分别观测两个变量得到的信息量应该和同时观测两个变量的信息量是相同的，我们用 $h()$ 来表示信息量，即：$h(x+y) = h(x) + h(y)$

> 来源：
>
> 1. https://zhuanlan.zhihu.com/p/25499358
> 2. https://zhuanlan.zhihu.com/p/30854084
> 3. http://www.ruanyifeng.com/blog/2019/08/information-theory.html

**成词标准一： 所处语境的丰富程度 - 熵 (Entropy)**  

通常我们认为两个片段可以成词的一个条件就是这个词语在很多语境中都出现。 
熵可以用来衡量这个指标。

熵是一种表示信息量的指标，熵越高就意味着信息量越大，不确定性越高，越难以预测。  

通常，对于一个随机变量 `X`，它的熵可以表示为：


$$
H(X) = - \sum_{x \in X}p(x)\log_2p(x)
$$


$p(x)$ 表示事件 `x` 出现的概率，在新词挖掘中就是一个词出现的概率。

$\log_2{p(x)} $ 表示的就是信息量。



通过计算一个候选词左边和右边的信息熵来反映一个词是否有丰富的左右搭配，如果达到一定的阀值，我们就认为这两个片段可以成为一个新词。

**成词标准二：内部聚合程度 - 互信息 (mutual information) & 点间互信息 (pointwise mutual information)**

**互信息 (MI)**：表示两个随机变量 `X`, `Y` 共享的信息量。

定义如下：


$$
MI(X,Y) = \sum_{x \in X, y \in Y} p(x,y)log_2 \frac{p(x,y)}{p(x)p(y)}
$$


**点间互信息**：表示两个事件之间的互信息。

定义如下：


$$
PMI(x,y) = \log_2 \frac{p(x,y)}{p(x)p(y)}
$$


当 `x`, `y` 相互独立时，`x` 跟 `y` 不相关，则 $p(x,y) = p(x)p(y)$，`PMI` 为 0。

如果两个相关性越大，则 $p(x,y)$ 就比 $p(x)p(y)$ 越大，那么 `PMI` 也越大。

`PMI` 通常被称为**凝合度**，数值越大表示两个片段一起出现的概率越大。



